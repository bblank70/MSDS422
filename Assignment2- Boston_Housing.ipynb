{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Supervised Techniques for Regression\n",
    "## Boston Housing Assignment\n",
    "\n",
    " Scikit Learn documentation for this assignment:\n",
    " http://scikit-learn.org/stable/modules/model_evaluation.html \n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    " http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    " http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "\n",
    " \n",
    " Textbook reference materials:\n",
    " Geron, A. 2017. Hands-On Machine Learning with Scikit-Learn\n",
    " and TensorFlow. Sebastopal, Calif.: O'Reilly. Chapter 3 Training Models\n",
    " has sections covering linear regression, polynomial regression,\n",
    " and regularized linear models. Sample code from the book is \n",
    " available on GitHub at https://github.com/ageron/handson-ml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements for this assignment\n",
    "\n",
    "    1) Use all explanatory variables (with the exception of neighborhood) and all 506 census tract observations from the Boston Housing Study.\n",
    "    2) Use one of two response variables: (1) the median value of homes in thousands of 1970 dollars or (2) the log median value of homes in thousands of 1970 dollars. \n",
    "    3) Employ at least two regression modeling methods selected from those discussed in Chapter 4 of the GÃ©ron (2017) textbook: linear regression, ridge regression, lasso regression, and elastic net.\n",
    "    4) Evaluate these methods within a cross-validation design, using root mean-squared error (RMSE) as an index of prediction error. \n",
    "\n",
    "\n",
    "Python scikit-learn should be your primary environment for conducting this research. Note that it is not necessary to employ polynomial regression in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Management problem:\n",
    "\n",
    "Imagine that you are advising a real estate brokerage firm in its attempt to employ machine learning methods. The firm wants to use machine learning to complement conventional methods for assessing the market value of residential real estate. Of the modeling methods examined in your study, which would you recommend to management, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "After getting our data and libraries loaded in section A. Section B is dedicated to performing some exploratory data analysis. Some of data are transformed here to adjust the shape of the values relative to our dependant variable (log_MV). There were a few things I was aiming to do with this assignment the first was to evaluate the. The first was to imagine myself as a scientist with tranditional statistical approaches; EDA followed by selecting the most significant features, and contrast that to a modern machine learning route where the more impactful features are identified. The second was to evaluate the statistics generated by a few different methodologis (linear regression versus ElasticNet and linear regression versus lasso).\n",
    "\n",
    "As for the first objective (section C) a subset of the columns were selected via exploratory analysis. Linear regression and elastic net are evaluated on the subset data. It is evident from the results below that these two methodologies produce very similar results in terms of coefficient estimates and term significance. To contrast this historical approach (section D), lasso regression was employed to identify the features which were most impactful. After the important features were identified, linear regression was used to contrast the results obtained from the lasso methodology. Again the results obtained were very similar interms of coefficients, error, and feature significance. Likewise as an additonal example a linear regression of all availble terms was conducted demonstrating the presence of overfitting of the model evidenced by the low significance of a number of terms. \n",
    "\n",
    "A key learning from this exercise is that machine learning outperforms more traditional approaches. For example when a scientist selects hypothesized impactful features they may have inherent bias because they lack the ability to see the data in the appropriate multidimensional space. Evaluating the data in a tranditional sense results in a model with a mean 8 fold CV RMSE score of ~0.22 in the scale of our dependant varaible. Conversely the most impactful features identified via lasso and subsequently evaluated with linear regression have an 8 fold CV RMSE score of ~0.20 allowing for a small reduction in the RMSE associated with the dependant variable improving our certainty and potentially allowing for larger profits with real estate investment by the firm. An additional learning is the power of lasso to downselect features for linear regression as the regularization methodology reduces the impact of the coefficient for a given feature to essentially zero. As such I would recommend this approach.\n",
    "\n",
    "Within each regression section (with the exception of linear regression performed on all features; section D2) the following generally applies:\n",
    "    \n"
   ]
  },
  {
   "source": [
    "    1) The train and test data are split from the data of interest\n",
    "    2) The model is fit on the training set using a pipeline\n",
    "    3) Predictions are made on the traning set and a parity plot is generated of predictions vs observed values\n",
    "    4) The training set is then passed to cross validation and RMSE is reported of each fold along with the average of all folds\n",
    "    5) Addiitonal Summary statistics for the coefficients and overal model are generated - Training Statistics\n",
    "    6) The model for the test set is fit using the same pipeline employed for the training set\n",
    "    7) Predictions are made on the test set and a parity plot is generated of predictions vs observed values\n",
    "    8) Addiitonal summary statistics for the coefficients and overal model are generated - Test Statistics\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A: Libraries and data read-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "# although we standardize X and y variables on input,\n",
    "# we will fit the intercept term in the models\n",
    "# Expect fitted values to be close to zero\n",
    "SET_FIT_INTERCEPT = True\n",
    "\n",
    "# import base packages into the namespace for this program\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt  # for root mean-squared error calculation\n",
    "\n",
    "#diplay and plotting\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "\n",
    "#import from SKlearn\n",
    "import sklearn.linear_model \n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# read data for the Boston Housing Study\n",
    "boston_input = pd.read_csv(\"C:/Users/bblank/Documents/Northwestern MSDS/datasets/boston.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to yield information about the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "   # mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "   # print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "\n",
    " \n",
    "#This function print's statistics in a nice table format for linear regression\n",
    "#requires the use of dataframe as inputs for all except the coefs arg.\n",
    "def model_statistics(x_actual, y_actual, y_predict, coefs):\n",
    "\n",
    "    matX = pd.DataFrame({\"Constant\":np.ones(len(x_actual))}).join(pd.DataFrame(x_actual.reset_index(drop=True)))\n",
    "    MSE = (sum((y_actual-y_predict)**2))/(len(matX)-len(matX.columns))\n",
    "    \n",
    "\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(matX.T,matX)).diagonal())\n",
    "    \n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = coefs/ sd_b\n",
    "\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(matX)-len(matX.columns)-1))) for i in ts_b]\n",
    "    \n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    coefs = np.round(coefs,4)\n",
    "\n",
    "    SummaryDF = pd.DataFrame()\n",
    "    SummaryDF[\"Coefficients\"],SummaryDF[\"Standard Errors\"],SummaryDF[\"t values\"],SummaryDF[\"Probabilities\"] = [coefs,sd_b,ts_b,p_values]\n",
    "\n",
    "    return(SummaryDF)\n",
    "\n",
    "# This is a function which returns the fomula of the model with it's coefficients\n",
    "def pretty_print_coefs(coefs, names = None, sort = False):\n",
    "    if names == None:\n",
    "        names = [\"X%s\" % x for x in range(len(coefs))]\n",
    "        lst = zip(coefs, names)\n",
    "    if sort:\n",
    "        lst = sorted(lst,  key = lambda x:-np.abs(x[0]))\n",
    "    return \" + \".join(\"%s * %s\" % (round(coef, 3), name) \n",
    "        for coef, name in lst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B: Descriptive Stats and Dataframe inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pandas DataFrame object boston_input\n",
    "print('\\nboston DataFrame (first and last five rows):')\n",
    "display(boston_input.head())\n",
    "display(boston_input.tail())\n",
    "\n",
    "print('\\n\\nGeneral description of the boston_input DataFrame:\\n')\n",
    "display(boston_input.info())\n",
    "\n",
    "# drop neighborhood from the data being considered\n",
    "boston = boston_input.drop('neighborhood', 1)\n",
    "print('\\n\\nGeneral description of the boston DataFrame:\\n')\n",
    "\n",
    "display(boston.info())\n",
    "\n",
    "print('\\nDescriptive statistics of the boston DataFrame:\\n')\n",
    "display(boston.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "#correlation matrix\n",
    "corr =boston.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.subplots_adjust(top=1)\n",
    "plt.suptitle(\"Correlation Heatmap for Boston\",fontsize= 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(boston.ptratio, boston.tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(boston.tax, np.log(boston.mv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_ratio= 1/boston.ptratio\n",
    "inv_ratio.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.lstat.hist(bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = np.sqrt(boston.lstat)\n",
    "\n",
    "log.hist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.rooms.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(boston.mv).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logplot = sns.scatterplot(y=np.log(boston.mv), x=boston.lstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrtfeatureplot = sns.scatterplot(y=np.log(boston.mv), x= np.sqrt(boston.lstat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrtfeatureplot = sns.scatterplot(y=boston.mv, x= 1/boston.nox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C: Scientist's approach; select correlated features\n",
    "## C1.) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_trimmed = boston[['mv', 'nox', 'lstat', 'ptratio', 'rooms', 'tax']]\n",
    "\n",
    "boston_trimmed['sqrt_lstat'] = np.sqrt(boston_trimmed['lstat'])\n",
    "boston_trimmed['log_MV']= np.log(boston_trimmed['mv'])\n",
    "\n",
    "boston_trimmed.drop(['lstat', 'mv'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "#correlation matrix\n",
    "corr =boston_trimmed.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.subplots_adjust(top=1)\n",
    "plt.suptitle(\"Correlation Heatmap for Boston_trimmed\",fontsize= 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here the scientist selects features sqrt_lstat, tax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the data used in the scientist selected features\n",
    "model_data =boston_trimmed[['log_MV','sqrt_lstat', 'tax']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trimmed_Train, Trimmed_Test = train_test_split(model_data, test_size = 0.2, random_state = RANDOM_SEED)\n",
    "\n",
    "#separate features from labels\n",
    "\n",
    "T_Test_y = Trimmed_Test['log_MV'].copy()\n",
    "T_Test_X = Trimmed_Test.drop('log_MV', axis=1)\n",
    "\n",
    "T_Train_y = Trimmed_Train['log_MV'].copy()\n",
    "T_Train_X = Trimmed_Train.drop('log_MV', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C2.) Linear Regression of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model\n",
    "Linear_pipe.fit(T_Train_X, T_Train_y)\n",
    "\n",
    "#predict the labels from the train values\n",
    "Linear_T_train_pred = Linear_pipe.predict(T_Train_X)\n",
    "\n",
    "#parity plot\n",
    "fig, T_train = plt.subplots()\n",
    "T_train.scatter(T_Train_y, Linear_T_train_pred)\n",
    "T_train.plot([T_Train_y.min(), T_Train_y.max()], [T_Train_y.min(), T_Train_y.max()], 'k--', lw=4)\n",
    "T_train.set_xlabel('Measured')\n",
    "T_train.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Stats (Linear Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression statistics functions\n",
    "regression_results(T_Train_y, Linear_T_train_pred)\n",
    "\n",
    "linear_coefs = np.append(Linear_pipe.named_steps['regressor'].intercept_, Linear_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=T_Train_X, y_actual=T_Train_y, y_predict=Linear_T_train_pred, coefs=linear_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold construction\n",
    "kfold = KFold(n_splits=8, random_state=RANDOM_SEED)\n",
    "\n",
    "#get RMSE for the folds\n",
    "cv_results = np.sqrt(-cross_val_score(Linear_pipe, T_Train_X, T_Train_y, cv=kfold, scoring= 'neg_mean_squared_error'))\n",
    "print(cv_results)\n",
    "np.mean(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our model on test data\n",
    "Linear_pipe.fit(T_Test_X, T_Test_y)\n",
    "\n",
    "#make predictions of the test labels\n",
    "Linear_T_test_pred =Linear_pipe.predict(T_Test_X)\n",
    "\n",
    "#parity plot\n",
    "fig, Linear_test = plt.subplots()\n",
    "Linear_test.scatter(T_Test_y, Linear_T_test_pred)\n",
    "Linear_test.plot([T_Train_y.min(), T_Train_y.max()], [T_Train_y.min(), T_Train_y.max()], 'k--', lw=4)\n",
    "Linear_test.set_xlabel('Measured')\n",
    "Linear_test.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary Stats (Linear Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our summary statistic functions\n",
    "regression_results(T_Test_y, Linear_T_test_pred)\n",
    "\n",
    "linear_coefs = np.append(Linear_pipe.named_steps['regressor'].intercept_, Linear_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=T_Test_X, y_actual=T_Test_y, y_predict=Linear_T_test_pred, coefs=linear_coefs)"
   ]
  },
  {
   "source": [
    "## C3.) Scientist Approach ElasticNet Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', ElasticNet(alpha=0.005, l1_ratio= 0.5))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model with EN on train\n",
    "Elastic_pipe.fit(T_Train_X, T_Train_y)\n",
    "\n",
    "#predict our outcomes of train\n",
    "Elastic_T_train_pred = Elastic_pipe.predict(T_Train_X)\n",
    "\n",
    "#parity plot\n",
    "fig, Elastic_train = plt.subplots()\n",
    "Elastic_train.scatter(T_Train_y, Linear_T_train_pred)\n",
    "Elastic_train.plot([T_Train_y.min(), T_Train_y.max()], [T_Train_y.min(), T_Train_y.max()], 'k--', lw=4)\n",
    "Elastic_train.set_xlabel('Measured')\n",
    "Elastic_train.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### ElasticNet Model Summary Stats (Train)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression statistics functions again\n",
    "regression_results(T_Train_y, Elastic_T_train_pred)\n",
    "\n",
    "EN_coefs = np.append(Elastic_pipe.named_steps['regressor'].intercept_, Elastic_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=T_Train_X, y_actual=T_Train_y, y_predict=Elastic_T_train_pred, coefs=EN_coefs)"
   ]
  },
  {
   "source": [
    "### Cross validation ElasticNet (Train)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidation\n",
    "cv_results = np.sqrt(-cross_val_score(Elastic_pipe, T_Train_X, T_Train_y, cv=kfold, scoring= 'neg_mean_squared_error'))\n",
    "print(cv_results)\n",
    "np.mean(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the test data using EN\n",
    "Elastic_pipe.fit(T_Test_X, T_Test_y)\n",
    "\n",
    "#predict our test labels using EN\n",
    "Elastic_T_test_pred = Elastic_pipe.predict(T_Test_X)\n",
    "\n",
    "#pairty plot\n",
    "fig, Elastic_test = plt.subplots()\n",
    "Elastic_test.scatter(T_Test_y, Elastic_T_test_pred)\n",
    "Elastic_test.plot([T_Train_y.min(), T_Train_y.max()], [T_Train_y.min(), T_Train_y.max()], 'k--', lw=4)\n",
    "Elastic_test.set_xlabel('Measured')\n",
    "Elastic_test.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "### ElasticNet model summary statistics (Test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our regression statistics again\n",
    "regression_results(T_Test_y, Elastic_T_test_pred)\n",
    "\n",
    "EN_coefs = np.append(Elastic_pipe.named_steps['regressor'].intercept_, Elastic_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=T_Test_X, y_actual=T_Test_y, y_predict=Elastic_T_test_pred, coefs=EN_coefs)\n"
   ]
  },
  {
   "source": [
    "# D. Machine Learning Approach\n",
    "## D1.) Data Prep"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get our data back in order\n",
    "boston_new =boston.copy()\n",
    "boston_new['sqrt_lstat'] = np.sqrt(boston_new['lstat'])\n",
    "boston_new['log_MV']= np.log(boston_new['mv'])\n",
    "\n",
    "boston_new.drop(['lstat', 'mv'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data now that we have all features again\n",
    "Train, Test = train_test_split(boston_new, test_size = 0.2, random_state = RANDOM_SEED)\n",
    "\n",
    "#separate features from labels for test\n",
    "init_Test_Labels = Test['log_MV'].copy()\n",
    "init_Test_Features = Test.drop('log_MV', axis=1)\n",
    "\n",
    "#separate features from labels for train\n",
    "init_y = Train['log_MV'].copy()\n",
    "init_X = Train.drop('log_MV', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D2.) Lasso Regression on Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=0.1))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit our data using lasso\n",
    "lasso_pipe.fit(init_X, init_y)\n",
    "lasso_Xtrain_pred =lasso_pipe.predict(init_X)\n",
    "\n",
    "#pairty plot\n",
    "fig, Lasso_train = plt.subplots()\n",
    "Lasso_train.scatter(init_y, lasso_Xtrain_pred)\n",
    "Lasso_train.plot([init_y.min(), init_y.max()], [init_y.min(), init_y.max()], 'k--', lw=4)\n",
    "Lasso_train.set_xlabel('Measured')\n",
    "Lasso_train.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Training Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression statistics functions\n",
    "regression_results(init_y, lasso_Xtrain_pred)\n",
    "\n",
    "lasso_coefs = np.append(lasso_pipe.named_steps['regressor'].intercept_, lasso_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=init_X, y_actual=init_y, y_predict=lasso_Xtrain_pred, coefs=lasso_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso impactful features\n",
    "    most impactful features (in descending order) are: sqrt_lstat, crim, rooms, ptratio (this last one is pretty small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our model formula\n",
    "print (\"Lasso model:\", pretty_print_coefs(lasso_pipe.named_steps['regressor'].coef_))\n"
   ]
  },
  {
   "source": [
    "#corresponding features for the model formula\n",
    "init_X.columns"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Cross Validation of the train data using lasso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = np.sqrt(-cross_val_score(lasso_pipe, init_X, init_y, cv=kfold, scoring= 'neg_mean_squared_error'))\n",
    "print(cv_results)\n",
    "np.mean(cv_results)"
   ]
  },
  {
   "source": [
    "### Lasso evaluation on test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit lasso on test data\n",
    "lasso_pipe.fit(init_Test_Features, init_Test_Labels)\n",
    "\n",
    "#predict test labels using lasso\n",
    "Lasso_test_pred = lasso_pipe.predict(init_Test_Features)\n",
    "\n",
    "#parity plot\n",
    "fig, Lasso_test = plt.subplots()\n",
    "Lasso_test.scatter(init_Test_Labels, Lasso_test_pred)\n",
    "Lasso_test.plot([init_Test_Labels.min(), init_Test_Labels.max()], [init_Test_Labels.min(), init_Test_Labels.max()], 'k--', lw=4)\n",
    "Lasso_test.set_xlabel('Measured')\n",
    "Lasso_test.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Model Statistics (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model statistics functions\n",
    "lasso_Test_pred = lasso_pipe.predict(init_Test_Features)\n",
    "regression_results(init_Test_Labels, lasso_Test_pred)\n",
    "\n",
    "lasso_coefs = np.append(lasso_pipe.named_steps['regressor'].intercept_, lasso_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual=init_Test_Features, y_actual=init_Test_Labels, y_predict=lasso_Test_pred, coefs=lasso_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2.) LinearRegression"
   ]
  },
  {
   "source": [
    "## D2a.) Linear Regression on Train (all features)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "        ])"
   ]
  },
  {
   "source": [
    "#fit all features with linear regressor\n",
    "Linear_pipe.fit(init_X, init_y)\n",
    "\n",
    "#predict labels on all features \n",
    "Linear_Xsub_train_pred =Linear_pipe.predict(init_X)\n",
    "\n",
    "#parity plot\n",
    "fig, Lin_train = plt.subplots()\n",
    "Lin_train.scatter(init_y, Linear_Xsub_train_pred)\n",
    "Lin_train.plot([init_y.min(), init_y.max()], [init_y.min(), init_y.max()], 'k--', lw=4)\n",
    "Lin_train.set_xlabel('Measured')\n",
    "Lin_train.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Linear Regression Statistics (all features)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model statistics functions\n",
    "regression_results(init_y, Linear_Xsub_train_pred)\n",
    "\n",
    "sub_coefs = np.append(Linear_pipe.named_steps['regressor'].intercept_, Linear_pipe.named_steps['regressor'].coef_)\n",
    "\n",
    "model_statistics(x_actual= init_X, y_actual= init_y, y_predict=Linear_Xsub_train_pred, coefs=sub_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this we can see that R2 is higher (RMSE also lower) for linear regression. \n",
    "    1) However, there are insignificant terms in our model indicated by low t-stats and high p-values in the summary table above.\n",
    "    2) The regularization of the lasso regressior penalizes the contribution of those coefficients such that it sets them to zero.\n",
    "    \n",
    "### Let's use the most prominant features from lasso regression (those with the largest coefficients) for our linear regression"
   ]
  },
  {
   "source": [
    "## D2b.) Linear Regression with lasso downselected features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_subset = Train[['sqrt_lstat', 'crim' , 'rooms', 'log_MV']]\n",
    "\n",
    "X_sub = boston_subset.iloc[:,0:3]\n",
    "y_sub = boston_subset.iloc[:,-1]\n",
    "\n",
    "X_sub.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_pipe.fit(X_sub, y_sub)\n",
    "\n",
    "Linear_Xsub_train_pred =Linear_pipe.predict(X_sub)\n",
    "\n",
    "fig, train = plt.subplots()\n",
    "train.scatter(y_sub, Linear_Xsub_train_pred)\n",
    "#plt.plot(Train_Features, Train_Labels_pred, color='blue', linewidth=3)\n",
    "train.plot([y_sub.min(), y_sub.max()], [y_sub.min(), y_sub.max()], 'k--', lw=4)\n",
    "train.set_xlabel('Measured')\n",
    "train.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Downselected Linear Regression model statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "regression_results(y_sub,Linear_Xsub_train_pred)\n",
    "sub_coefs = np.append(Linear_pipe.named_steps['regressor'].intercept_, Linear_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual= X_sub, y_actual= y_sub, y_predict=Linear_Xsub_train_pred, coefs=sub_coefs)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=8, random_state=RANDOM_SEED)\n",
    "\n",
    "#get RMSE for the folds\n",
    "cv_results = np.sqrt(-cross_val_score(Linear_pipe, X_sub, y_sub, cv=kfold, scoring= 'neg_mean_squared_error'))\n",
    "print(cv_results)\n",
    "np.mean(cv_results)"
   ]
  },
  {
   "source": [
    "## implement the identified feature downselect for linear regression on the test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_subset = Test[['sqrt_lstat', 'crim' , 'rooms','log_MV']]\n",
    "Test_X_sub = Test_subset.iloc[:,0:3]\n",
    "Test_y_sub = Test_subset.iloc[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear_pipe.fit(Test_X_sub, Test_y_sub)\n",
    "\n",
    "Linear_Xsub_test_pred =Linear_pipe.predict(Test_X_sub)\n",
    "\n",
    "fig, Test_ln = plt.subplots()\n",
    "Test_ln.scatter(Test_y_sub, Linear_Xsub_test_pred)\n",
    "#plt.plot(Train_Features, Train_Labels_pred, color='blue', linewidth=3)\n",
    "Test_ln.plot([Test_y_sub.min(), Test_y_sub.max()], [Test_y_sub.min(), Test_y_sub.max()], 'k--', lw=4)\n",
    "Test_ln.set_xlabel('Measured')\n",
    "Test_ln.set_ylabel('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Summary Statistis for downselected linear regression (test)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Noregression_results(y_sub,Linear_Xsub_train_pred)\n",
    "regression_results(Test_y_sub,Linear_Xsub_test_pred)\n",
    "test_sub_coefs = np.append(Linear_pipe.named_steps['regressor'].intercept_, Linear_pipe.named_steps['regressor'].coef_)\n",
    "model_statistics(x_actual= Test_X_sub, y_actual= Test_y_sub, y_predict=Linear_Xsub_test_pred, coefs=test_sub_coefs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}